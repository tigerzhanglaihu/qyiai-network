import qyiutil_network
import panel as pn
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import param
import logging
import json  # å¯¼å…¥ json æ¨¡å—ä»¥è§£æ JSON å“åº”

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class State(param.Parameterized):
    """
    ä¿å­˜ç¨‹åºçš„å…¨å±€çŠ¶æ€ï¼ŒåŒ…æ‹¬æœ¬åœ°æœºå™¨äººç¯å¢ƒã€ç›®æ ‡æ–‡ä»¶å¤¹ã€æ¨¡å‹åç§°ç­‰ã€‚
    """
    instenv = qyiutil_network.networkbot()  # åˆå§‹åŒ–æœ¬åœ°æœºå™¨äººç¯å¢ƒ
    target_folder = instenv.VECTOR_DB_PATH  # è®¾ç½®ç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
    model_name = instenv.LLM_OLLAMA_MODEL  # è®¾ç½®LLMæ¨¡å‹åç§°
    upload_file = param.Bytes()  # å‚æ•°åŒ–ä¸Šä¼ æ–‡ä»¶
    embeddingsmodel_name = instenv.EMBEDDING_MODEL_PATH + '/' + instenv.EMBEDDING_MODEL  # è®¾ç½®åµŒå…¥æ¨¡å‹è·¯å¾„

# åˆå§‹åŒ–æœºå™¨äººçŠ¶æ€å’ŒèŠå¤©å†å²
state = State()
chat_history = []

# è·å–å‘é‡æ•°æ®åº“é€‰é¡¹
vectordbs_options = qyiutil_network.get_vectordbs(state.instenv.VECTOR_DB_PATH)

# åˆ›å»ºå‘é‡æ•°æ®åº“é€‰æ‹©å™¨
chain_type_input = pn.widgets.RadioButtonGroup(
    name="æœ¬åœ°å‘é‡æ•°æ®åº“åç§°",
    options=vectordbs_options,
    orientation="vertical",
    sizing_mode="stretch_width",
    button_type="primary",
    button_style="outline",
)

# å®šä¹‰è¾“å…¥æ–‡æœ¬æ¡†å’Œæ–‡ä»¶è¾“å…¥æ¡†
text_input = pn.widgets.TextInput(name="", placeholder="è¾“å…¥ä½ æƒ³æé—®çš„é—®é¢˜ï¼")
file_input = pn.widgets.FileInput(name="ä¸Šä¼ æ–‡ä»¶", accept=".pdf,.csv,.txt")


# å®šä¹‰è·å–æœºå™¨äººå›åº”çš„å‡½æ•°
def _get_response(contents):
    """
    è·å–æœºå™¨äººå›åº”ï¼Œæ ¹æ®æ˜¯å¦é€‰æ‹©äº†å‘é‡æ•°æ®åº“æ¥å†³å®šå›åº”çš„æ–¹å¼ã€‚
    """
    logging.info("è·å–æœºå™¨äººå›åº”")
    logging.info(f"chain_type_input.value: {str(chain_type_input.value)}")
    logging.info(f"contents: {str(contents)}")

    if chain_type_input.value is None:
        llm = ChatOpenAI(
            api_key= state.instenv.LLM_NETWORK_KEY, # å¦‚æœæ‚¨æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨æ­¤å¤„ç”¨æ‚¨çš„API Keyè¿›è¡Œæ›¿æ¢
            base_url=state.instenv.LLM_NETWORK_URL, # å¡«å†™DashScope base_url
            model=state.instenv.LLM_OLLAMA_MODEL
            )
        messages = [
            SystemMessage(content="You are a helpful assistant."),
            HumanMessage(content=contents),
        ]
        response = llm.invoke(messages)
        content_value = response.json(ensure_ascii=False)
        response = json.loads(content_value)
        response = response['content']
        chat_history.append((contents, response))
    else:
        qa = qyiutil_network._get_retrieval_qa(chain_type_input.value, state.instenv.VECTOR_DB_PATH,state.instenv.LLM_NETWORK_KEY,
                                               state.instenv.LLM_NETWORK_URL, state.instenv.LLM_OLLAMA_MODEL, state.instenv.EMBEDDING_MODEL_PATH,
                                               state.instenv.EMBEDDING_MODEL)

        response = qa.invoke({"question": contents, "chat_history": chat_history})
        chat_history.append((contents, response["answer"]))
        response = response["answer"]

    return response, []

# å®šä¹‰å“åº”ç”¨æˆ·è¾“å…¥çš„å‡½æ•°
async def respond(contents, user, chat_interface):
    """
    å“åº”ç”¨æˆ·è¾“å…¥çš„å‡½æ•°ï¼Œå¤„ç†ç”¨æˆ·çš„æ–‡æœ¬è¾“å…¥å’Œæ–‡ä»¶ä¸Šä¼ ã€‚
    """
    if chat_interface.active == 0:
        response, _ = _get_response(contents)
        yield {"user": "Qyi", "avatar": "ğŸ¤–ï¸", "object": response}
    elif chat_interface.active == 1:
        chat_interface.active = 0
        yield {"user": "Qyi", "avatar": "ğŸ¤–ï¸", "object": "æ–‡ä»¶æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ç¨å...."}

        upload_message = qyiutil_network._save_file(contents, chat_interface, state.instenv.FILE_UPLOAD_PATH,
                                            state.instenv.VECTOR_DB_PATH, state.instenv.EMBEDDING_MODEL_PATH,
                                            state.instenv.EMBEDDING_MODEL, '')
        # æ›´æ–°å‘é‡æ•°æ®åº“é€‰é¡¹
        chain_type_input.options = qyiutil_network.get_vectordbs(state.instenv.VECTOR_DB_PATH)
        yield {"user": "Qyi", "avatar": "ğŸ¤–ï¸", "object": "æ–‡ä»¶ä¸Šä¼ å®Œæˆ," + str(upload_message)}

# åˆ›å»ºèŠå¤©ç•Œé¢
chat_interface = pn.chat.ChatInterface(
    widgets=[text_input, file_input],
    callback=respond,
    callback_exception='verbose',
    sizing_mode="stretch_width",
    renderers=pn.pane.Perspective,
    show_rerun=False,
    show_undo=False,
    show_clear=True,
)

# å‘é€æ¬¢è¿æ¶ˆæ¯
chat_interface.send(
    "QyiAI-ç½‘ç»œç‰ˆ 1.0",
    user="system",
    respond=False
)

# è·å–å½“å‰æ¨¡å‹çš„æè¿°
model_desc = 'æ¨¡å‹ï¼š' + state.model_name

# åˆ›å»º Bootstrap æ¨¡æ¿
template = pn.template.BootstrapTemplate(
    title="QyiAi-ç½‘ç»œç‰ˆ-V1.0",
    sidebar=[model_desc, chain_type_input],
    main=[chat_interface],
)

# æ˜¾ç¤ºæ¨¡æ¿
template.servable()
template.show()

logging.info("QyiAI-çŸ¥è¯†åº“ç½‘ç»œç‰ˆå¯åŠ¨æˆåŠŸ")
